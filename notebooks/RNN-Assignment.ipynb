{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNTGk8Y3yTvbovImy6jzOqs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"sHLITCEM1wxD","executionInfo":{"status":"ok","timestamp":1715661871871,"user_tz":-330,"elapsed":4808,"user":{"displayName":"Gowthamaan","userId":"02052851975593809583"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset, Dataset\n","import pandas as pd\n","import numpy as np\n","import re\n","import random\n","\n"]},{"cell_type":"markdown","source":["https://github.com/bentrevett/pytorch-seq2seq/tree/main\n","\n","https://github.com/pankajrawat9075/English-Hindi-Language-Transliteration-using-Deep-Learning/tree/main\n","\n","https://github.com/biku1998/Neural-machine-transliteration-using-Pytorch/tree/master"],"metadata":{"id":"Vjc6_RuKkY4d"}},{"cell_type":"code","source":["# set the device we will be using to train the model\n","device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"LInpKpDmCsRv","executionInfo":{"status":"ok","timestamp":1715661871871,"user_tz":-330,"elapsed":13,"user":{"displayName":"Gowthamaan","userId":"02052851975593809583"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IFKSZRy9Ctt_","executionInfo":{"status":"ok","timestamp":1715661871872,"user_tz":-330,"elapsed":13,"user":{"displayName":"Gowthamaan","userId":"02052851975593809583"}},"outputId":"e0577c77-d24b-420d-b824-c9224c41c4da"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=1)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# storing all the alphabets of English and the pad char to a dictionary to create OHE representation later.\n","eng_alphabets = 'abcdefghijklmnopqrstuvwxyz'\n","pad_char = '-PAD-'\n","\n","eng_alpha2index = {pad_char: 0}\n","for index, alpha in enumerate(eng_alphabets):\n","    eng_alpha2index[alpha] = index+1\n","\n","print(eng_alpha2index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-fxu6O-gKWm","executionInfo":{"status":"ok","timestamp":1715661871872,"user_tz":-330,"elapsed":10,"user":{"displayName":"Gowthamaan","userId":"02052851975593809583"}},"outputId":"48ef279d-b68c-4f51-cff5-ff7751e1d8b1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{'-PAD-': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"]}]},{"cell_type":"code","source":["# Tamil Unicode Hex Range is 2944:3071. Source: https://en.wikipedia.org/wiki/Tamil_(Unicode_block)\n","tamil_alphabets = [chr(alpha) for alpha in range(2944, 3072)]\n","tamil_alphabet_size = len(tamil_alphabets)\n","\n","tamil_alpha2index = {0:pad_char}\n","for index, alpha in enumerate(tamil_alphabets):\n","    tamil_alpha2index[index+1] = alpha\n","\n","print(tamil_alpha2index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPR5PHpKictW","executionInfo":{"status":"ok","timestamp":1715662012478,"user_tz":-330,"elapsed":3,"user":{"displayName":"Gowthamaan","userId":"02052851975593809583"}},"outputId":"d282b627-b7ee-4b77-e0d8-b3c66f4bde1b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '-PAD-', 1: '\\u0b80', 2: '\\u0b81', 3: 'ஂ', 4: 'ஃ', 5: '\\u0b84', 6: 'அ', 7: 'ஆ', 8: 'இ', 9: 'ஈ', 10: 'உ', 11: 'ஊ', 12: '\\u0b8b', 13: '\\u0b8c', 14: '\\u0b8d', 15: 'எ', 16: 'ஏ', 17: 'ஐ', 18: '\\u0b91', 19: 'ஒ', 20: 'ஓ', 21: 'ஔ', 22: 'க', 23: '\\u0b96', 24: '\\u0b97', 25: '\\u0b98', 26: 'ங', 27: 'ச', 28: '\\u0b9b', 29: 'ஜ', 30: '\\u0b9d', 31: 'ஞ', 32: 'ட', 33: '\\u0ba0', 34: '\\u0ba1', 35: '\\u0ba2', 36: 'ண', 37: 'த', 38: '\\u0ba5', 39: '\\u0ba6', 40: '\\u0ba7', 41: 'ந', 42: 'ன', 43: 'ப', 44: '\\u0bab', 45: '\\u0bac', 46: '\\u0bad', 47: 'ம', 48: 'ய', 49: 'ர', 50: 'ற', 51: 'ல', 52: 'ள', 53: 'ழ', 54: 'வ', 55: 'ஶ', 56: 'ஷ', 57: 'ஸ', 58: 'ஹ', 59: '\\u0bba', 60: '\\u0bbb', 61: '\\u0bbc', 62: '\\u0bbd', 63: 'ா', 64: 'ி', 65: 'ீ', 66: 'ு', 67: 'ூ', 68: '\\u0bc3', 69: '\\u0bc4', 70: '\\u0bc5', 71: 'ெ', 72: 'ே', 73: 'ை', 74: '\\u0bc9', 75: 'ொ', 76: 'ோ', 77: 'ௌ', 78: '்', 79: '\\u0bce', 80: '\\u0bcf', 81: 'ௐ', 82: '\\u0bd1', 83: '\\u0bd2', 84: '\\u0bd3', 85: '\\u0bd4', 86: '\\u0bd5', 87: '\\u0bd6', 88: 'ௗ', 89: '\\u0bd8', 90: '\\u0bd9', 91: '\\u0bda', 92: '\\u0bdb', 93: '\\u0bdc', 94: '\\u0bdd', 95: '\\u0bde', 96: '\\u0bdf', 97: '\\u0be0', 98: '\\u0be1', 99: '\\u0be2', 100: '\\u0be3', 101: '\\u0be4', 102: '\\u0be5', 103: '௦', 104: '௧', 105: '௨', 106: '௩', 107: '௪', 108: '௫', 109: '௬', 110: '௭', 111: '௮', 112: '௯', 113: '௰', 114: '௱', 115: '௲', 116: '௳', 117: '௴', 118: '௵', 119: '௶', 120: '௷', 121: '௸', 122: '௹', 123: '௺', 124: '\\u0bfb', 125: '\\u0bfc', 126: '\\u0bfd', 127: '\\u0bfe', 128: '\\u0bff'}\n"]}]},{"cell_type":"code","source":["word = [8, 34, 28, 80, 24, 77, 50, 80, 50, 65]\n","word = [24, 79, 39, 49, 65, 44, 80]\n","decoded_word = \"\".join([tamil_alpha2index.get(char-2) for char in word])\n","decoded_word"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"uzigdF6Uarj2","executionInfo":{"status":"ok","timestamp":1715662177811,"user_tz":-330,"elapsed":6,"user":{"displayName":"Gowthamaan","userId":"02052851975593809583"}},"outputId":"0358ea21-bbd1-46dc-c735-28fddcd25c3c"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'கௌதமான்'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["**Helper Functions**"],"metadata":{"id":"MjFNwa02knF_"}},{"cell_type":"code","source":["# Funcitons used to do some pre-processing.\n","# removing all non-alphabetic char in English as well as Tamil.\n","\n","non_eng_letters_regex = re.compile('[^a-zA-Z ]')\n","\n","# Remove all English non-letters\n","def cleanEnglishVocab(line):\n","    line = line.replace('-', ' ').replace(',', ' ').lower()\n","    line = non_eng_letters_regex.sub('', line)\n","    return line.split()\n","\n","# Remove all Tamil non-letters\n","def cleanTamilVocab(line):\n","    line = line.replace('-', ' ').replace(',', ' ')\n","    cleaned_line = ''\n","    for char in line:\n","        if char in tamil_alpha2index or char == ' ':\n","            cleaned_line += char\n","    return cleaned_line.split()"],"metadata":{"id":"jjaXIsd0kSYu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Dataloader**"],"metadata":{"id":"YSxp834Ckw34"}},{"cell_type":"code","source":["class TransliterationDataLoader(Dataset):\n","    def __init__(self, filename):\n","        self.eng_words, self.hindi_words = self.readXmlDataset(filename, cleanTamilVocab)\n","        self.shuffle_indices = list(range(len(self.eng_words)))\n","        random.shuffle(self.shuffle_indices)\n","        self.shuffle_start_index = 0\n","\n","    def __len__(self):\n","        return len(self.eng_words)\n","\n","    def __getitem__(self, idx):\n","        return self.eng_words[idx], self.hindi_words[idx]\n","\n","    def readXmlDataset(self, filename, lang_vocab_cleaner):\n","        '''Task : to read the xml file and store all the contents in a list.\n","                  Then we will do some pre-processing of data to remove noise as well as delimeters. '''\n","        transliterationCorpus = pd.read_csv(filename,  header=None)\n","        en_words, ta_words = transliterationCorpus.iloc[:, 0], transliterationCorpus.iloc[:, 1]\n","\n","        lang1_words = []\n","        lang2_words = []\n","\n","        for idx in range(len(en_words)):\n","            wordlist1 = cleanEnglishVocab(en_words[idx]) # clean English words.\n","            wordlist2 = cleanTamilVocab(ta_words[idx])# clean hindi words.\n","\n","            # Skip noisy data\n","            if len(wordlist1) != len(wordlist2):\n","                print('Skipping: ', en_words[idx], ' - ', ta_words[idx])\n","                continue\n","\n","            for word in wordlist1:\n","                lang1_words.append(word)\n","            for word in wordlist2:\n","                lang2_words.append(word)\n","\n","        return lang1_words, lang2_words\n","\n","    def get_random_sample(self):\n","        return self.__getitem__(np.random.randint(len(self.eng_words)))\n","\n","    def get_batch_from_array(self, batch_size, array): # child function of get_batch() function.\n","        '''Given an array , and batch size , this fucntion will return some samples from the array i.e can be HindiWords or EnglishWords etc. '''\n","        end = self.shuffle_start_index + batch_size # what index till i want to go.\n","        batch = []\n","        if end >= len(self.eng_words): # if we overflow the words array , we have to loop back.\n","            batch = [array[i] for i in self.shuffle_indices[0:end%len(self.eng_words)]]\n","            end = len(self.eng_words)\n","        return batch + [array[i] for i in self.shuffle_indices[self.shuffle_start_index : end]]\n","\n","    def get_batch(self, batch_size, postprocess = True):\n","        eng_batch = self.get_batch_from_array(batch_size, self.eng_words)\n","        hindi_batch = self.get_batch_from_array(batch_size, self.hindi_words)\n","        self.shuffle_start_index += batch_size + 1\n","\n","        # Reshuffle if 1 epoch is complete\n","        if self.shuffle_start_index >= len(self.eng_words):\n","            random.shuffle(self.shuffle_indices)\n","            self.shuffle_start_index = 0\n","\n","        return eng_batch, hindi_batch"],"metadata":{"id":"oLRDTiHakwf1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = TransliterationDataLoader('tam_train.csv')\n","val_data = TransliterationDataLoader('tam_valid.csv')"],"metadata":{"id":"VrNtOl57kfG6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Basic Data Visualization**"],"metadata":{"id":"Mao6JNxkCH0A"}},{"cell_type":"code","source":["print(\"Train Set Size:\\t\", len(train_data))\n","print(\"Validation Set Size:\\t\", len(val_data))\n","\n","print('\\nSample data from train-set:')\n","for i in range(10):\n","    eng, tam = train_data.get_random_sample()\n","    print(eng + ' - ' + tam)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEwi15puCLZP","executionInfo":{"status":"ok","timestamp":1715437401600,"user_tz":-330,"elapsed":372,"user":{"displayName":"Gowthamaan","userId":"02052851975593809583"}},"outputId":"e4c8bf56-9e6c-4e09-83f6-03c9948cf485"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Set Size:\t 51200\n","Validation Set Size:\t 4096\n","\n","Sample data from train-set:\n","ravipudram - ரவிபுத்ரம்\n","anumathikkuriya - அனுமதிக்குரிய\n","sumikkasha - சுமிக்கஷா\n","inthappulligalaal - இந்தப்புள்ளிகளால்\n","ubaga - உபக\n","kavalaikkollaathirukka - கவலைக்கொள்ளாதிருக்க\n","maiyavaatham - மையவாதம்\n","stalinukumtan - ஸ்டாலினுக்கும்தான்\n","avasiyamaakkiya - அவசியமாக்கிய\n","kannalmozhi - கன்னல்மொழி\n"]}]},{"cell_type":"code","source":["train_data.get_batch(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13CDiWlJCcrR","executionInfo":{"status":"ok","timestamp":1715437445888,"user_tz":-330,"elapsed":3,"user":{"displayName":"Gowthamaan","userId":"02052851975593809583"}},"outputId":"bd1b9a62-28ee-4f3f-9c56-8db796c3d674"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['anandrathu',\n","  'aalaakiyullaen',\n","  'padiganga',\n","  'malaiyaalaththirkum',\n","  'valia',\n","  'kurippidaththodangiya',\n","  'iruthiyaattralaiyum',\n","  'vaikkappaduvathundu',\n","  'puumaraangk',\n","  'evidae'],\n"," ['அனன்றது',\n","  'ஆளாகியுள்ளேன்',\n","  'பாடிகங்கா',\n","  'மலையாளத்திற்கும்',\n","  'வாலியா',\n","  'குறிப்பிடத்தொடங்கிய',\n","  'இறுதியாற்றலையும்',\n","  'வைக்கப்படுவதுண்டு',\n","  'பூமராங்க்',\n","  'எவிடே'])"]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["**Sequence to Sequence Modelling**"],"metadata":{"id":"qYIF0nXXC0oo"}},{"cell_type":"code","source":["class Seq2SeqModel(nn.Module):\n","    def __init__(self, input_size, output_size, embedding_dim, hidden_dim, num_layers, cell_type='lstm', bidirectional=False, dropout=0.0):\n","        super(Seq2SeqModel, self).__init__()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.cell_type = cell_type\n","        self.bidirectional = bidirectional\n","        self.dropout = dropout\n","\n","        self.embedding = nn.Embedding(input_size, embedding_dim)\n","\n","        if cell_type == 'rnn':\n","            self.encoder = nn.RNN(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=dropout)\n","            self.decoder = nn.RNN(hidden_dim * (2 if bidirectional else 1), hidden_dim, num_layers, dropout=dropout)\n","        elif cell_type == 'lstm':\n","            self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=dropout)\n","            self.decoder = nn.LSTM(hidden_dim * (2 if bidirectional else 1), hidden_dim, num_layers, dropout=dropout)\n","        elif cell_type == 'gru':\n","            self.encoder = nn.GRU(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=dropout)\n","            self.decoder = nn.GRU(hidden_dim * (2 if bidirectional else 1), hidden_dim, num_layers, dropout=dropout)\n","\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","\n","    def forward(self, input_seq, hidden=None, beam_size=1):\n","        batch_size = input_seq.size(1)\n","        seq_length = input_seq.size(0)\n","\n","        embedded = self.embedding(input_seq)\n","\n","        outputs, hidden = self.encoder(embedded, hidden)\n","\n","        if self.bidirectional:\n","            hidden = (hidden[0].view(self.num_layers, 2, batch_size, self.hidden_dim)[:, :1].contiguous(),\n","                      hidden[1].view(self.num_layers, 2, batch_size, self.hidden_dim)[:, :1].contiguous())\n","        else:\n","            hidden = (hidden[0].view(self.num_layers, 1, batch_size, self.hidden_dim),\n","                      hidden[1].view(self.num_layers, 1, batch_size, self.hidden_dim))\n","\n","        if beam_size > 1:\n","            hidden = (hidden[0].repeat(1, beam_size, 1, 1),\n","                      hidden[1].repeat(1, beam_size, 1, 1))\n","            batch_size = batch_size * beam_size\n","\n","        decoder_input = torch.zeros(1, batch_size, self.hidden_dim * (2 if self.bidirectional else 1), device=input_seq.device)\n","        decoder_outputs = []\n","\n","        hypotheses = torch.zeros(batch_size, beam_size, seq_length, device=input_seq.device).long()\n","        scores = torch.zeros(batch_size, beam_size, device=input_seq.device)\n","\n","        for t in range(seq_length):\n","            output, hidden = self.decoder(decoder_input, hidden)\n","            decoder_output = self.fc(output.squeeze(0))\n","            decoder_outputs.append(decoder_output)\n","\n","            if beam_size > 1:\n","                decoder_output = decoder_output.view(batch_size, beam_size, -1)\n","                log_probs = torch.log_softmax(decoder_output, dim=-1)\n","                scores = scores.unsqueeze(-1) + log_probs\n","                scores, indices = scores.view(batch_size, -1).topk(beam_size, dim=-1)\n","                hypotheses[:, :, t] = indices % self.output_size\n","                decoder_input = hypotheses[:, :, t].view(1, batch_size * beam_size, -1)\n","            else:\n","                decoder_input = output\n","\n","        if beam_size > 1:\n","            decoder_outputs = [output.view(batch_size, beam_size, -1) for output in decoder_outputs]\n","            decoder_outputs = torch.stack(decoder_outputs, dim=2)\n","            decoder_outputs = decoder_outputs.transpose(1, 2)\n","        else:\n","            decoder_outputs = torch.stack(decoder_outputs)\n","\n","        return decoder_outputs"],"metadata":{"id":"VmD4w7Hg18uG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define a custom dataset class\n","class TransliterationDataset(Dataset):\n","    def __init__(self, csv_file):\n","        self.data = pd.read_csv(csv_file)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        src_word = self.data.iloc[idx, 0]\n","        target_word = self.data.iloc[idx, 1]\n","\n","        # Convert words to tensors (you might need additional preprocessing)\n","        # Example: Convert words to indices using some kind of vocabulary mapping\n","        # For simplicity, let's assume the words are already converted to indices\n","        src_tensor = torch.tensor(src_word)\n","        target_tensor = torch.tensor(target_word)\n","\n","        return src_tensor, target_tensor"],"metadata":{"id":"z4RRilEdBTuI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load data from CSV files\n","train_data = TransliterationDataset(\"tam_train.csv\")\n","val_data = TransliterationDataset(\"tam_valid.csv\")\n","test_data = TransliterationDataset(\"tam_test.csv\")\n","\n","# Create DataLoaders\n","batch_size = 32\n","train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=batch_size)\n","test_loader = DataLoader(test_data, batch_size=batch_size)"],"metadata":{"id":"v6yaA8KI98G5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_size = 256  # Size of the input character vocabulary\n","output_size = 256  # Size of the output character vocabulary\n","embedding_dim = 128  # Dimension of the character embeddings\n","hidden_dim = 512  # Dimension of the hidden state in the encoder and decoder RNNs\n","num_layers = 2  # Number of layers in the encoder and decoder RNNs\n","cell_type = 'lstm'  # Type of RNN cell ('rnn', 'lstm', or 'gru')\n","bidirectional = True  # Whether the encoder should be bidirectional\n","dropout = 0.2  # Dropout rate"],"metadata":{"id":"0DGddMieCcNb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate the model\n","model = Seq2SeqModel(input_size, output_size, embedding_dim, hidden_dim, num_layers, cell_type, bidirectional, dropout)\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss(ignore_index=0)  # Assuming 0 is the padding index\n","optimizer = optim.Adam(model.parameters())"],"metadata":{"id":"k279WuK0Cfgh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10"],"metadata":{"id":"mMakBVgdCwmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    epoch_loss = 0.0\n","\n","    for inputs, targets in train_loader:\n","        print(inputs)\n","        print(targets)\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        outputs = outputs.view(-1, outputs.size(-1))\n","        targets = targets.view(-1)\n","\n","        # Calculate loss\n","        loss = criterion(outputs, targets)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    # Validation loop\n","    model.eval()\n","    val_loss = 0.0\n","\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","\n","            # Forward pass with beam search\n","            beam_size = 4  # Example beam size\n","            outputs = model(inputs, beam_size=beam_size)\n","\n","            # Calculate loss (you may need to modify this based on your evaluation metric)\n","            outputs = outputs.view(-1, outputs.size(-1))\n","            targets = targets.view(-1)\n","            loss = criterion(outputs, targets)\n","\n","            val_loss += loss.item()\n","\n","    train_loss = epoch_loss / len(train_loader)\n","    val_loss = val_loss / len(val_loader)\n","\n","    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n","\n","# Save the trained model\n","torch.save(model.state_dict(), 'model.pth')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"svcGgUfW21Rw","executionInfo":{"status":"error","timestamp":1715153461841,"user_tz":-330,"elapsed":16,"user":{"displayName":"Gowthamaan","userId":"02052851975593809583"}},"outputId":"07d8bfd5-65af-4ee9-fd2c-e811db8d8223"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"new(): invalid data type 'str'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-04ecbfbbe58e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-eaf7e20bbc71>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Example: Convert words to indices using some kind of vocabulary mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# For simplicity, let's assume the words are already converted to indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msrc_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: new(): invalid data type 'str'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jK4P5XPgC2JL"},"execution_count":null,"outputs":[]}]}